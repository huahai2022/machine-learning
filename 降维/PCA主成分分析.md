# 降维概述

降维就是使用某种映射方法，将原来高维空间的数据点映射到低纬度的空间。降维的本质是学习一个映射函数f:x->y，其中x是原始数据点的表达，且多为向量表达式，y是数据点映射后的低维空间表达，通常y的维度比x的维度小。使用降维的原因是，在样本中的高维空间中，有一些冗余信息和噪音，造成误差，降低了准确率；通过降维，我们希望减少冗余信息造成的误差，提高精度。或者使用降维算法识别数据内部的本质特征。

降维算法可以作为数据预处理的一种，如PCA，事实上，有些算法如果没有降维预处理，效果是不好的。

# PCA引出

PCA就是找到一个投影矩阵，将数据从高维投影到低维，我们可以把这个低维子空间当作一个超平面，什么样的投影是一个好的投影呢，PCA认为这个超平面应该满足下面的条件：

- 最近重构性：样本点到这个超平面的举例都足够近
- 最大可分性：样本点在超平面上的投影尽可能分散

假设我们有m个样本(x~1~,x~2~,……x~m~)，且数据的维度为N。

## 方差

我们希望投影值尽可能分散，这种分散程度可以通过方差进行描述，设投影后的样本为a，即a=xw，x的形状是(m,n)，m个样本，n个特征，w的形状是(n,n-1)，那么a的形状是(m,n-1)，实现了降维。方差公式为：
$$
Var(a)=\frac{1}{m}\sum_{i=1}^{m}(a_i-u)^2
$$
为了方便处理，我们先对样本进行减均值操作，这样u=0，上式可写为：
$$
Var(a)=\frac{1}{m}\sum_{i=1}^{m}a_i^2
$$

## 协方差

对于高维问题，最大化方差相当于优化目标
$$
max tr(W^TXX^TW)
$$
tr（）表示求矩阵的秩。在高维空间的投影中，与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向，如果我们单纯选择方差最大的方向，这个方向与第一个方向重合，显然这样是没用的，因此，应该有其它的约束条件。

一个比较直观的想法是投影后的特征各个维度之间的相关性比较低，最好线性无关，两个分布大的相关性可以通过相关系数来描述，如皮尔逊相关系数：
$$
r(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var[X]Var[Y]}}
$$

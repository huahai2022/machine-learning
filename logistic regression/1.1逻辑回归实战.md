# sklearn.linear_model.LogisticRegression

链接直达：[sklearn.linear_model.LogisticRegression — scikit-learn 1.5.dev0 documentation](https://scikit-learn.org/dev/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)

下面我们来看一看模型的参数和方法。

![image-20240308170122743](https://dradon.oss-cn-hangzhou.aliyuncs.com/img/image-20240308170122743.png)

## 参数

- **penalty：指定正则化项的范数。**可选值有：'l1'（L1正则化），'l2'（L2正则化），'elasticnet'（弹性网络正则化）和None（无正则化）。默认为'l2'。其中，L1正则化会促使模型参数稀疏化，L2正则化会使参数趋向于较小的值，弹性网络正则化结合了L1和L2正则化。

- dual：表示是否使用对偶（受限）或原始（正则化）形式进行求解。对偶形式仅适用于使用liblinear求解器的L2正则化。当样本数大于特征数时，建议设置dual=False。默认为False。

- tol：停止标准的容差。当模型的变化小于容差时，停止迭代。默认为1e-4。

- **C：正则化强度的倒数。**必须是正的浮点数。较小的值对应更强的正则化。类似于支持向量机中的惩罚参数。默认为1.0。

- fit_intercept：是否在决策函数中添加常数项（截距）。默认为True。

- intercept_scaling：仅在使用'liblinear'求解器且fit_intercept设置为True时有用。在这种情况下，输入向量x会变为[x，self.intercept_scaling]。即向实例向量添加一个“合成”特征，该特征的常数值等于intercept_scaling。截距变为intercept_scaling * synthetic_feature_weight。

- class_weight：类别权重，可以是一个字典{class_label: weight}或字符串'balanced'。如果不提供，则假定所有类别的权重都为1。"balanced"模式会根据输入数据中每个类别的频率自动调整权重，使其与样本的类别频率成反比。

- random_state：用于在solver为'sag'、'saga'或'liblinear'时对数据进行洗牌。它接受一个整数或RandomState实例作为参数。

- **solver：优化问题中使用的算法。**默认为'lbfgs'。选择求解器时，需要考虑以下几个方面：
  - 对于小数据集，'liblinear'是一个不错的选择，而对于大数据集，'sag'和'saga'更快；
  
  - 对于多类别问题，只有'newton-cg'、'sag'、'saga'和'lbfgs'支持多项式损失；
  
  - 'liblinear'仅支持一对多策略。
  
  - 对于样本数远大于特征数的情况，特别是具有稀有类别的独热编码分类特征，'newton-cholesky'是一个不错的选择。请注意，它仅适用于二元分类和多类别分类的一对多归约。要注意的是，该求解器的内存使用量与特征数的平方成正比，因为它显式计算Hessian矩阵。

- **max_iter：求解器收敛的最大迭代次数。**默认为100。如果求解器在达到最大迭代次数之前已经收敛，则会提前停止。

- multi_class：多类别分类的方式。可选值有'auto'、'ovr'和'multinomial'。如果选择'ovr'，则为每个标签拟合一个二元分类问题。如果选择'multinomial'，则最小化的损失是在整个概率分布上拟合的多项式损失，即使数据是二元的。当solver='liblinear'时，不可用'multinomial'选项。'auto'选项会根据数据是否二元或solver是否为'liblinear'来自动选择分类方式。

- verbose：对于liblinear和lbfgs求解器，设置verbose为正数以打印详细信息。

- warm_start：当设置为True时，重用上一次fit调用的解作为初始化，否则，只是清除上一次的解。对于liblinear求解器无效。详见术语表。

- n_jobs：如果multi_class='ovr'，则在类别并行化时使用的CPU核心数。当求解器设置为'liblinear'时，无论是否指定了'multi_class'，此参数都会被忽略。None表示使用1个核心，除非在joblib.parallel_backend上下文中。-1表示使用所有处理器。有关更多详细信息，请参阅术语表。

- l1_ratio：Elastic-Net混合参数，取值范围为0 <= l1_ratio <= 1。仅在penalty='elasticnet'时使用。设置l1_ratio=0等效于使用penalty='l2'，而设置l1_ratio=1等效于使用penalty='l1'。对于0 < l1_ratio < 1，惩罚项是L1和L2的组合。

其中常用的一些参数，使用粗体进行表明。

## 方法

![image-20240308170845988](https://dradon.oss-cn-hangzhou.aliyuncs.com/img/image-20240308170845988.png)

下面对方法进行介绍

- decision_function(X)：为样本预测分类的置信度得分。返回一个数组，数组的形状为(n_samples, )，其中n_samples是输入样本的数量。

- densify()：将系数矩阵转换为密集数组格式。

- **fit(X, y[, sample_weight])：**根据给定的训练数据对模型进行拟合。

- get_metadata_routing()：获取此对象的元数据路由。

- get_params([deep])：获取此估计器的参数。

- **predict(X)：**对样本X进行分类预测，返回预测的类标签。

- predict_log_proba(X)：预测样本X的对数概率估计。

- **predict_proba(X)：**预测样本X的概率估计。

- **score(X, y[, sample_weight])：**返回给定测试数据和标签的平均准确率。

- set_fit_request(*[, sample_weight])：请求传递给fit方法的元数据。

- set_params(**params)：设置此估计器的参数。

- set_score_request(*[, sample_weight])：请求传递给score方法的元数据。

- sparsify()：将系数矩阵转换为稀疏格式。

这些方法提供了对逻辑回归分类器进行训练、预测和评估的功能，并可以操作模型的参数和元数据。常用的方法同样使用黑体进行显示。

下面我们先使用逻辑回归识别鸢尾花数据集做个开胃菜。

# 逻辑回归分类鸢尾花

给出下面代码，可以看到全部分类正确。

```python
'''
@File    :     logistic regression.py    
@Contact :     zhangzhilong2022@gmail.com
@Modify Time   2024/3/8 17:47   
@Author        huahai2022
@Desciption
'''
# -*- coding: utf-8 -*-

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.metrics import accuracy_score
# 加载sklearn内置的iris鸢尾花数据集
iris = datasets.load_iris()

X = iris.data
y = iris.target

print('Sample num: ', len(y))#150

# 将原始数据集随机切分成两部分，作为训练集和测试集，其中测试集占总样本30%。
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 设置模型参数并使用训练集训练模型。
clf = LogisticRegression(C=1.0, penalty='l2',tol=1e-6)
print(clf)
# 训练模型
clf.fit(X_train, y_train)

# 使用测试集预测结果
ans = clf.predict(X_test)

print("Accuracy:",accuracy_score(y_test,ans))
# 100%
```

# 逻辑回归识别手写数字

给出下面的代码：这里给出的最大迭代次数为10000，因为我尝试了其它的值，模型没有达到收敛的效果。

```python
'''
@File    :     logistic regression2.py    
@Contact :     zhangzhilong2022@gmail.com
@Modify Time   2024/3/8 17:54   
@Author        huahai2022
@Desciption
'''
from matplotlib import pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
# 加载MNIST数据集
digits = datasets.load_digits()
X = digits.data
print(X.shape)
y = digits.target
print(y.shape)

plt.imshow(X[0].reshape(8, 8),cmap='gray')
plt.show()
# 将数据集拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model =  LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
```

